{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d91838-8db2-40f2-8af5-af21481c673a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/huggingface/diffusers.git\n",
    "%cd diffusers\n",
    "!pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00834327-5f5a-474f-93e9-c56b2f44a1e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/diffusers\n",
      "Processing /home/jupyter/diffusers\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: importlib_metadata in /opt/conda/lib/python3.10/site-packages (from diffusers==0.32.0.dev0) (8.4.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from diffusers==0.32.0.dev0) (3.16.1)\n",
      "Collecting huggingface-hub>=0.23.2 (from diffusers==0.32.0.dev0)\n",
      "  Using cached huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from diffusers==0.32.0.dev0) (1.25.2)\n",
      "Collecting regex!=2019.12.17 (from diffusers==0.32.0.dev0)\n",
      "  Using cached regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from diffusers==0.32.0.dev0) (2.32.3)\n",
      "Collecting safetensors>=0.3.1 (from diffusers==0.32.0.dev0)\n",
      "  Using cached safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from diffusers==0.32.0.dev0) (11.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers==0.32.0.dev0) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers==0.32.0.dev0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers==0.32.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers==0.32.0.dev0) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers==0.32.0.dev0) (4.12.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib_metadata->diffusers==0.32.0.dev0) (3.20.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers==0.32.0.dev0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers==0.32.0.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers==0.32.0.dev0) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers==0.32.0.dev0) (2024.8.30)\n",
      "Using cached huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Using cached regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "Using cached safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Building wheels for collected packages: diffusers\n",
      "  Building wheel for diffusers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for diffusers: filename=diffusers-0.32.0.dev0-py3-none-any.whl size=2977364 sha256=766de093a1883dbbd94a7c7c69c99d37c120ab524403b6f541574c39c36656f2\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-rnsfv3_0/wheels/57/60/f2/fe2861f83705358f706e41236370c4f2bc56e6c5b9aff95aee\n",
      "Successfully built diffusers\n",
      "Installing collected packages: safetensors, regex, huggingface-hub, diffusers\n",
      "Successfully installed diffusers-0.32.0.dev0 huggingface-hub-0.26.2 regex-2024.11.6 safetensors-0.4.5\n"
     ]
    }
   ],
   "source": [
    "%cd diffusers\n",
    "!pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d15ade8-f0a0-4378-823c-6317ddce9d84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/diffusers/examples/text_to_image\n",
      "Collecting accelerate>=0.16.0 (from -r requirements.txt (line 1))\n",
      "  Using cached accelerate-1.1.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (0.15.1+cu118)\n",
      "Collecting transformers>=4.25.1 (from -r requirements.txt (line 3))\n",
      "  Using cached transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting datasets>=2.19.1 (from -r requirements.txt (line 4))\n",
      "  Using cached datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting ftfy (from -r requirements.txt (line 5))\n",
      "  Using cached ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting tensorboard (from -r requirements.txt (line 6))\n",
      "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: Jinja2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (3.1.4)\n",
      "Collecting peft==0.7.0 (from -r requirements.txt (line 8))\n",
      "  Using cached peft-0.7.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.7.0->-r requirements.txt (line 8)) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.7.0->-r requirements.txt (line 8)) (24.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.7.0->-r requirements.txt (line 8)) (5.9.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.7.0->-r requirements.txt (line 8)) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.7.0->-r requirements.txt (line 8)) (2.0.0+cu118)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft==0.7.0->-r requirements.txt (line 8)) (4.66.5)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.7.0->-r requirements.txt (line 8)) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.7.0->-r requirements.txt (line 8)) (0.26.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->-r requirements.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->-r requirements.txt (line 2)) (11.0.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 8)) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 8)) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 8)) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 8)) (3.4.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 8)) (2.0.0)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 8)) (3.30.5)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 8)) (18.1.8)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.25.1->-r requirements.txt (line 3)) (2024.11.6)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers>=4.25.1->-r requirements.txt (line 3))\n",
      "  Using cached tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=2.19.1->-r requirements.txt (line 4))\n",
      "  Using cached pyarrow-18.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.19.1->-r requirements.txt (line 4))\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.19.1->-r requirements.txt (line 4)) (2.0.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.19.1->-r requirements.txt (line 4)) (3.5.0)\n",
      "Collecting multiprocess<0.70.17 (from datasets>=2.19.1->-r requirements.txt (line 4))\n",
      "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.19.1->-r requirements.txt (line 4))\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.19.1->-r requirements.txt (line 4)) (3.9.5)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from ftfy->-r requirements.txt (line 5)) (0.2.13)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 6)) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 6)) (1.67.1)\n",
      "Collecting markdown>=2.6.8 (from tensorboard->-r requirements.txt (line 6))\n",
      "  Using cached Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 6)) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 6)) (75.1.0)\n",
      "Requirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 6)) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->-r requirements.txt (line 6))\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard->-r requirements.txt (line 6))\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2->-r requirements.txt (line 7)) (3.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.19.1->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.19.1->-r requirements.txt (line 4)) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.19.1->-r requirements.txt (line 4)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.19.1->-r requirements.txt (line 4)) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.19.1->-r requirements.txt (line 4)) (1.17.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.19.1->-r requirements.txt (line 4)) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r requirements.txt (line 2)) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r requirements.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r requirements.txt (line 2)) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r requirements.txt (line 2)) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.19.1->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.19.1->-r requirements.txt (line 4)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.19.1->-r requirements.txt (line 4)) (2024.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp->datasets>=2.19.1->-r requirements.txt (line 4)) (0.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 8)) (1.3.0)\n",
      "Using cached peft-0.7.0-py3-none-any.whl (168 kB)\n",
      "Using cached accelerate-1.1.1-py3-none-any.whl (333 kB)\n",
      "Using cached transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "Using cached datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "Using cached ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Using cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Using cached Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Using cached pyarrow-18.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.1 MB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Using cached tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Installing collected packages: werkzeug, tensorboard-data-server, pyarrow, markdown, ftfy, fsspec, dill, tensorboard, multiprocess, tokenizers, transformers, datasets, accelerate, peft\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 14.0.2\n",
      "    Uninstalling pyarrow-14.0.2:\n",
      "      Successfully uninstalled pyarrow-14.0.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
      "ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 18.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.1.1 datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 ftfy-6.3.1 markdown-3.7 multiprocess-0.70.16 peft-0.7.0 pyarrow-18.1.0 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tokenizers-0.20.3 transformers-4.46.3 werkzeug-3.1.3\n"
     ]
    }
   ],
   "source": [
    "%cd examples/text_to_image\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd4bea-7778-416e-b594-ad52cf001f7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls /home/jupyter/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb17eb0-e795-4e21-bf5c-e827864abc0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil -m cp -r gs://text-to-image-429503/flickr8k/Images/* /home/jupyter/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15041a51-d907-495a-bf30-b7ef2a268ec3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# Paths\n",
    "images_folder = \"/home/jupyter/dataset\"  \n",
    "metadata_path=os.path.join(images_folder, \"metadata.jsonl\")\n",
    "output_folder = \"/home/jupyter/shortened_dataset\"\n",
    "shortened_metadata_path = os.path.join(output_folder, \"metadata.jsonl\")\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Processing\n",
    "unique_images = set()\n",
    "shortened_metadata = []\n",
    "\n",
    "with open(metadata_path, \"r\") as metadata_file:\n",
    "    for line in metadata_file:\n",
    "        entry = json.loads(line.strip())\n",
    "        file_name = entry[\"file_name\"]\n",
    "\n",
    "        # Add to unique images until 200 unique file_names are reached\n",
    "        if len(unique_images) < 200:\n",
    "            if file_name not in unique_images:\n",
    "                unique_images.add(file_name)\n",
    "                # Copy the corresponding image to the new folder\n",
    "                src_path = os.path.join(images_folder, file_name)\n",
    "                dst_path = os.path.join(output_folder, file_name)\n",
    "                if os.path.exists(src_path):\n",
    "                    shutil.copy(src_path, dst_path)\n",
    "                else:\n",
    "                    print(f\"Warning: Image file {src_path} not found!\")\n",
    "\n",
    "            # Add the entry to the new metadata\n",
    "            shortened_metadata.append(entry)\n",
    "\n",
    "# Write the shortened metadata to a new JSONL file\n",
    "with open(shortened_metadata_path, \"w\") as output_file:\n",
    "    for entry in shortened_metadata:\n",
    "        output_file.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(\"Dataset culling complete. Shortened metadata and images stored in /shortened_dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "687148ea-c25e-4062-8c6f-2e1aafeacbd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:XRT configuration not detected. Defaulting to preview PJRT runtime. To silence this warning and continue using PJRT, explicitly set PJRT_DEVICE to a supported device or configure XRT. To disable default device selection, set PJRT_SELECT_DEFAULT_DEVICE=0\n",
      "WARNING:root:For more information about the status of PJRT, see https://github.com/pytorch/xla/blob/master/docs/pjrt.md\n",
      "WARNING:root:Defaulting to PJRT_DEVICE=CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], device='xla:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "dev = xm.xla_device()\n",
    "t1 = torch.ones(3, 3, device=dev)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60392038-2efb-4fbf-9a02-53364dc94650",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r /home/jupyter/model\n",
    "!mkdir /home/jupyter/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4709566b-20eb-450a-8c90-9249c1b02e79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "INFO:__main__:Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "{'variance_type', 'timestep_spacing', 'clip_sample_range', 'dynamic_thresholding_ratio', 'prediction_type', 'sample_max_value', 'rescale_betas_zero_snr', 'thresholding'} was not found in config. Values will be initialized to default values.\n",
      "{'norm_num_groups', 'latents_std', 'latents_mean', 'shift_factor', 'use_quant_conv', 'use_post_quant_conv', 'force_upcast', 'mid_block_add_attention'} was not found in config. Values will be initialized to default values.\n",
      "{'transformer_layers_per_block', 'projection_class_embeddings_input_dim', 'resnet_skip_time_act', 'time_embedding_type', 'encoder_hid_dim', 'time_cond_proj_dim', 'addition_time_embed_dim', 'use_linear_projection', 'reverse_transformer_layers_per_block', 'resnet_out_scale_factor', 'only_cross_attention', 'mid_block_type', 'cross_attention_norm', 'addition_embed_type_num_heads', 'mid_block_only_cross_attention', 'upcast_attention', 'resnet_time_scale_shift', 'dropout', 'timestep_post_act', 'addition_embed_type', 'encoder_hid_dim_type', 'conv_in_kernel', 'dual_cross_attention', 'time_embedding_dim', 'time_embedding_act_fn', 'num_attention_heads', 'conv_out_kernel', 'attention_type', 'num_class_embeds', 'class_embed_type', 'class_embeddings_concat'} was not found in config. Values will be initialized to default values.\n",
      "{'transformer_layers_per_block', 'projection_class_embeddings_input_dim', 'resnet_skip_time_act', 'time_embedding_type', 'encoder_hid_dim', 'time_cond_proj_dim', 'addition_time_embed_dim', 'use_linear_projection', 'reverse_transformer_layers_per_block', 'resnet_out_scale_factor', 'only_cross_attention', 'mid_block_type', 'cross_attention_norm', 'addition_embed_type_num_heads', 'mid_block_only_cross_attention', 'upcast_attention', 'resnet_time_scale_shift', 'dropout', 'timestep_post_act', 'addition_embed_type', 'encoder_hid_dim_type', 'conv_in_kernel', 'dual_cross_attention', 'time_embedding_dim', 'time_embedding_act_fn', 'num_attention_heads', 'conv_out_kernel', 'attention_type', 'num_class_embeds', 'class_embed_type', 'class_embeddings_concat'} was not found in config. Values will be initialized to default values.\n",
      "Resolving data files: 100%|███████████████| 201/201 [00:00<00:00, 124878.55it/s]\n",
      "INFO:__main__:***** Running training *****\n",
      "INFO:__main__:  Num examples = 200\n",
      "INFO:__main__:  Num Epochs = 100\n",
      "INFO:__main__:  Instantaneous batch size per device = 48\n",
      "INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "INFO:__main__:  Gradient Accumulation steps = 1\n",
      "INFO:__main__:  Total optimization steps = 500\n",
      "Steps: 100%|████████| 500/500 [21:34<00:00,  2.10s/it, lr=1e-8, step_loss=0.136]INFO:accelerate.accelerator:Saving current state to /home/jupyter/model/checkpoint-500\n",
      "{'transformer_layers_per_block', 'projection_class_embeddings_input_dim', 'resnet_skip_time_act', 'time_embedding_type', 'encoder_hid_dim', 'time_cond_proj_dim', 'addition_time_embed_dim', 'use_linear_projection', 'reverse_transformer_layers_per_block', 'resnet_out_scale_factor', 'only_cross_attention', 'mid_block_type', 'cross_attention_norm', 'addition_embed_type_num_heads', 'mid_block_only_cross_attention', 'upcast_attention', 'resnet_time_scale_shift', 'dropout', 'timestep_post_act', 'addition_embed_type', 'encoder_hid_dim_type', 'conv_in_kernel', 'dual_cross_attention', 'time_embedding_dim', 'time_embedding_act_fn', 'num_attention_heads', 'conv_out_kernel', 'attention_type', 'num_class_embeds', 'class_embed_type', 'class_embeddings_concat'} was not found in config. Values will be initialized to default values.\n",
      "Configuration saved in /home/jupyter/model/checkpoint-500/unet_ema/config.json\n",
      "Model weights saved in /home/jupyter/model/checkpoint-500/unet_ema/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in /home/jupyter/model/checkpoint-500/unet/config.json\n",
      "Model weights saved in /home/jupyter/model/checkpoint-500/unet/diffusion_pytorch_model.safetensors\n",
      "INFO:accelerate.checkpointing:Optimizer state saved in /home/jupyter/model/checkpoint-500/optimizer.bin\n",
      "INFO:accelerate.checkpointing:Scheduler state saved in /home/jupyter/model/checkpoint-500/scheduler.bin\n",
      "INFO:accelerate.checkpointing:Sampler state for dataloader 0 saved in /home/jupyter/model/checkpoint-500/sampler.bin\n",
      "INFO:accelerate.checkpointing:Gradient scaler state saved in /home/jupyter/model/checkpoint-500/scaler.pt\n",
      "INFO:accelerate.checkpointing:Random states saved in /home/jupyter/model/checkpoint-500/random_states_0.pkl\n",
      "INFO:__main__:Saved state to /home/jupyter/model/checkpoint-500\n",
      "Steps: 100%|███████████| 500/500 [22:02<00:00,  2.10s/it, lr=0, step_loss=0.308]\n",
      "model_index.json: 100%|████████████████████████| 541/541 [00:00<00:00, 4.71MB/s]\u001b[A\n",
      "\n",
      "Fetching 10 files:   0%|                                 | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "(…)kpoints/scheduler_config-checkpoint.json: 100%|█| 209/209 [00:00<00:00, 1.92M\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "(…)ature_extractor/preprocessor_config.json: 100%|█| 342/342 [00:00<00:00, 4.03M\u001b[A\u001b[A\n",
      "\n",
      "Fetching 10 files:  10%|██▌                      | 1/10 [00:00<00:00,  9.07it/s]\u001b[A\n",
      "\n",
      "safety_checker/config.json: 100%|██████████| 4.56k/4.56k [00:00<00:00, 26.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:   0%|                            | 0.00/1.22G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   2%|▎                   | 21.0M/1.22G [00:00<00:08, 146MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   4%|▊                   | 52.4M/1.22G [00:00<00:05, 204MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   7%|█▍                  | 83.9M/1.22G [00:00<00:05, 217MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   9%|█▉                   | 115M/1.22G [00:00<00:04, 226MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  12%|██▌                  | 147M/1.22G [00:00<00:04, 231MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  15%|███                  | 178M/1.22G [00:00<00:04, 221MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  17%|███▌                 | 210M/1.22G [00:00<00:04, 222MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  20%|████▏                | 241M/1.22G [00:01<00:04, 221MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  22%|████▋                | 273M/1.22G [00:01<00:04, 228MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  25%|█████▎               | 304M/1.22G [00:01<00:04, 225MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  28%|█████▊               | 336M/1.22G [00:01<00:03, 222MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  30%|██████▎              | 367M/1.22G [00:01<00:04, 206MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  32%|██████▋              | 388M/1.22G [00:01<00:04, 205MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  34%|███████▏             | 419M/1.22G [00:02<00:04, 188MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  37%|███████▊             | 451M/1.22G [00:02<00:03, 197MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  40%|████████▎            | 482M/1.22G [00:02<00:03, 208MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  42%|████████▊            | 514M/1.22G [00:02<00:03, 216MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  45%|█████████▍           | 545M/1.22G [00:02<00:03, 217MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  47%|█████████▉           | 577M/1.22G [00:02<00:02, 221MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  50%|██████████▌          | 608M/1.22G [00:02<00:02, 224MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  53%|███████████          | 640M/1.22G [00:02<00:02, 227MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  55%|███████████▌         | 671M/1.22G [00:03<00:02, 226MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  58%|████████████▏        | 703M/1.22G [00:03<00:02, 227MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  60%|████████████▋        | 734M/1.22G [00:03<00:02, 231MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  63%|█████████████▏       | 765M/1.22G [00:03<00:01, 230MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  66%|█████████████▊       | 797M/1.22G [00:03<00:01, 234MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  68%|██████████████▎      | 828M/1.22G [00:03<00:01, 237MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  71%|██████████████▊      | 860M/1.22G [00:03<00:01, 237MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  73%|███████████████▍     | 891M/1.22G [00:04<00:01, 240MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  76%|███████████████▉     | 923M/1.22G [00:04<00:01, 237MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  78%|████████████████▍    | 954M/1.22G [00:04<00:01, 238MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  81%|█████████████████    | 986M/1.22G [00:04<00:00, 237MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  84%|████████████████▋   | 1.02G/1.22G [00:04<00:00, 234MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  86%|█████████████████▏  | 1.05G/1.22G [00:04<00:00, 236MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  89%|█████████████████▊  | 1.08G/1.22G [00:04<00:00, 236MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  91%|██████████████████▎ | 1.11G/1.22G [00:04<00:00, 237MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  94%|██████████████████▊ | 1.14G/1.22G [00:05<00:00, 240MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  97%|███████████████████▎| 1.17G/1.22G [00:05<00:00, 238MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors: 100%|████████████████████| 1.22G/1.22G [00:05<00:00, 225MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Fetching 10 files: 100%|████████████████████████| 10/10 [00:05<00:00,  1.78it/s]\u001b[A\n",
      "{'requires_safety_checker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[A{'timestep_spacing', 'prediction_type'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
      "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
      "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
      "\n",
      "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 20.71it/s]\u001b[A\n",
      "Configuration saved in /home/jupyter/model/vae/config.json\n",
      "Model weights saved in /home/jupyter/model/vae/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in /home/jupyter/model/unet/config.json\n",
      "Model weights saved in /home/jupyter/model/unet/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in /home/jupyter/model/scheduler/scheduler_config.json\n",
      "Configuration saved in /home/jupyter/model/model_index.json\n",
      "Steps: 100%|███████████| 500/500 [22:49<00:00,  2.74s/it, lr=0, step_loss=0.308]\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch train_text_to_image.py \\\n",
    "  --pretrained_model_name_or_path=CompVis/stable-diffusion-v1-4  \\\n",
    "  --train_data_dir=/home/jupyter/shortened_dataset  \\\n",
    "  --use_ema \\\n",
    "  --resolution=512 --center_crop --random_flip \\\n",
    "  --train_batch_size=48 \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --gradient_checkpointing \\\n",
    "  --mixed_precision=fp16 \\\n",
    "  --max_train_steps=500 \\\n",
    "  --learning_rate=5e-06 \\\n",
    "  --max_grad_norm=1 \\\n",
    "  --lr_scheduler=\"linear\" \\\n",
    "  --lr_warmup_steps=0\\\n",
    "  --output_dir=/home/jupyter/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26ca0b50-a0ca-4823-8888-a7143f78915a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ../../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b51ea38-be9b-4756-8dfd-aee2edd7a1a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDFinetune.ipynb  diffusers  new-workspace.jupyterlab-workspace  src\n",
      "dataset\t\t  model      shortened_dataset\t\t\t tutorials\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "933d5fb0-9d63-474e-be0e-6bed331c8ac0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560f8c59e4144e25b07e75ae23822344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7840e2ff3c094ef0aa60adb115b0f9b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "model_path = \"model\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16)\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "image = pipe(prompt=\"Dave\").images[0]\n",
    "image.save(\"test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9cd4a8a-03fa-4655-b066-265c024ffb99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://model/model_index.json [Content-Type=application/json]...\n",
      "Copying file://model/logs/text2image-fine-tune/events.out.tfevents.1732728687.5d19749c37f3.259.0 [Content-Type=application/octet-stream]...\n",
      "Copying file://model/logs/text2image-fine-tune/1732728687.324261/hparams.yml [Content-Type=application/octet-stream]...\n",
      "Copying file://model/logs/text2image-fine-tune/1732728687.3225884/events.out.tfevents.1732728687.5d19749c37f3.259.1 [Content-Type=application/octet-stream]...\n",
      "Copying file://model/unet/config.json [Content-Type=application/json]...        \n",
      "Copying file://model/unet/diffusion_pytorch_model.safetensors [Content-Type=application/octet-stream]...\n",
      "Copying file://model/tokenizer/vocab.json [Content-Type=application/json]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "Copying file://model/feature_extractor/preprocessor_config.json [Content-Type=application/json]...\n",
      "Copying file://model/checkpoint-500/scheduler.bin [Content-Type=application/octet-stream]...\n",
      "Copying file://model/tokenizer/special_tokens_map.json [Content-Type=application/json]...\n",
      "Copying file://model/tokenizer/tokenizer_config.json [Content-Type=application/json]...\n",
      "Copying file://model/tokenizer/merges.txt [Content-Type=text/plain]...          \n",
      "Copying file://model/checkpoint-500/optimizer.bin [Content-Type=application/octet-stream]...\n",
      "Copying file://model/checkpoint-500/random_states_0.pkl [Content-Type=application/octet-stream]...\n",
      "Copying file://model/checkpoint-500/scaler.pt [Content-Type=application/vnd.snesdev-page-table]...\n",
      "Copying file://model/checkpoint-500/unet/config.json [Content-Type=application/json]...\n",
      "Copying file://model/checkpoint-500/unet/diffusion_pytorch_model.safetensors [Content-Type=application/octet-stream]...\n",
      "Copying file://model/text_encoder/model.safetensors [Content-Type=application/octet-stream]...\n",
      "Copying file://model/checkpoint-500/unet_ema/diffusion_pytorch_model.safetensors [Content-Type=application/octet-stream]...\n",
      "Copying file://model/checkpoint-500/unet_ema/config.json [Content-Type=application/json]...\n",
      "Copying file://model/text_encoder/config.json [Content-Type=application/json]...\n",
      "Copying file://model/scheduler/scheduler_config.json [Content-Type=application/json]...\n",
      "Copying file://model/safety_checker/config.json [Content-Type=application/json]...\n",
      "Copying file://model/safety_checker/model.safetensors [Content-Type=application/octet-stream]...\n",
      "Copying file://model/vae/config.json [Content-Type=application/json]...         \n",
      "Copying file://model/vae/diffusion_pytorch_model.safetensors [Content-Type=application/octet-stream]...\n",
      "\\ [26/26 files][ 17.5 GiB/ 17.5 GiB] 100% Done 116.4 MiB/s ETA 00:00:00         \n",
      "Operation completed over 26 objects/17.5 GiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil -m cp -r model gs://text-to-image-model-429503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad4993c-d300-4cfb-8172-7736b0686ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.0",
   "language": "python",
   "name": "pytorch-2-0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
